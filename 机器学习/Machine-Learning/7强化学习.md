# 强化学习（Reinforcement Learning）

强化学习是一种机器学习方法，它通过代理与环境的交互来学习如何做出最优决策。代理在不断尝试中获得奖励信号，并通过调整策略来最大化累积奖励。这种学习方式类似于人类通过试错来学习，帮助机器代理逐步提高在特定任务上的表现能力。

强化学习算法的思路非常简单，以游戏为例，如果在游戏中采取某种策略可以取得较高的得分，那么就进一步“强化”这种策略，以期继续取得较好的结果。这种策略与日常生活中的各种“绩效奖励”非常类似。

# 离散状态空间

## 示例：火星探测器

探测器在火星执行探测任务，共有6个位置。在强化学习中，探测器的位置被称为状态，初始时，探测器位于状态4。
现在，位置1和位置6都有一些有趣的表面。科学家希望探测器对位置1进行采样，因为它比位置6更重要。但是位置1很远，所以我们通过奖励函数来反映状态的价值，在每个状态可以获得对应的奖励。状态1的奖励是100，状态6的奖励是40，其他状态的奖励都是0。在每一步中，探测器可以选择向左或向右移动。
![](assets/Pasted%20image%2020230721182005.png)
在每个位置，机器人都处于某种状态s，它可以选择一个动作a，并从而得到奖励R(s)，状态随动作变为新的状态s'。注意，奖励R(s)是与状态s相关，而不是下一个状态s'相关。
![](assets/Pasted%20image%2020230721182247.png)
例如，当探测器处于状态4，并采取了向左走的动作，状态4奖励为0，变为新的状态3。

强化学习四要素：**状态、动作、奖励、下一个状态**


探测器在不同的位置可以走不同的路线，起点为位置4可以走4-3-2-1，也可以走4-5-6。较远的位置虽然奖励多，但是它所花费的时间和路程值得这些奖励吗？怎么知道一组特定的奖励比另一组不同的奖励更好还是更差?

我们用强化学习中的**回报（Return）** 来解决这件事。

## 回报 

基于折扣因子和奖励值，计算一系列动作下所获得的总效用值。折扣因子 $\gamma$ (Gamma)：取值范围在0到1之间，一般是一个非常接近1的数值。
![](assets/Pasted%20image%2020230721184441.png)

得到的回报取决于奖励，而奖励取决于你采取的行动，因此回报取决于你采取的行动。
![](assets/Pasted%20image%2020230721185214.png)

## 策略

策略（policy）是指代理在特定状态下选择动作的方式或规则。策略定义了代理对于给定状态所做出的行为，就是一个从状态到行为的映射。我们可以通过策略来确定每个状态下的行为。

强化学习的目标是训练出一个策略函数，它以任意状态s为输入，并将其映射到采取的某个动作a。 

例如可以有4种策略，以最后一种策略为例，右边是策略函数
![](assets/Pasted%20image%2020230721194933.png)

强化学习的目标是找到一个策略 $\pi$ ，告诉你在每个状态下采取什么行动以最大化回报。

## 马尔可夫决策过程

**马尔可夫决策过程**（Markov Decision Process, MDP），指未来仅取决于当前状态，而不取决于当前状态之前发生的任何事情。换句话说，在马尔可夫决策过程中，未来只取决于你现在所处的位置，而不取决于你是如何到达这里的。
![](assets/Pasted%20image%2020230721202150.png)

根据目标当前状态 s，基于策略选择动作a，世界或环境会发生变化，然后观察基于世界状态，目标的状态s及得到的奖励R

## 状态动作价值函数Q(s,a)
状态动作价值函数 Q(s, a)（也称为 Q-value 函数）是在强化学习中用于评估在给定状态 s 下采取动作 a 的价值的函数。它表示了代理在特定状态下采取某个动作所能获得的预期累积回报。

具体而言，对于给定的状态动作对 (s, a)，Q(s, a) 表示从状态 s 开始，采取某一行动，之后一直按照最佳的策略行动，最终获得的预期累积回报。这个预期累积回报通常是通过累加未来奖励的折扣值来计算的。（至于为何能在获得最优策略前计算Q，后面会讲，会基于循环）。
![](assets/Pasted%20image%2020230721224150.png)

例如：
计算Q（2，→），则为以状态2开始，采取向右的行动，到状态3，那后面最优策略是一直向左走，行动轨迹为2-3-2-1，所以计算Q为12.5。
Q（2，←）则为以状态2开始，采取某一行动为向左，行动轨迹为2-1，此时Q为50。

对每个状态，每个动作，都可计算得到Q。而且我们可以发现，Q(s,a)的最大值即为从状态s获得的最佳可能回报。因此当我们可以计算Q(s,a)，即可知道最佳的动作。
![](assets/Pasted%20image%2020230721224959.png)

如果你可以计算每一个状态每一个动作的Q(s,a)，然后找出最大的Q(s,a)，这个Q(s,a)中的a即为最佳的行动，所以该状态s的 $\pi(s)=a$ 。

因此，Q计算和策略相对应，如果能基于s和a计算Q函数，就能得到策略。

## 贝尔曼方程
贝尔曼方程（Bellman equation）是强化学习中的关键方程，用来描述状态值函数或者状态动作值函数之间的递归关系。
**贝尔曼方程**：$Q(s,a)=R(s)+\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime})$ 

贝尔曼方程通过将**当前**状态和动作的价值与**下一个**状态的价值联系起来，帮助我们计算最优策略。也就是说，贝尔曼方程告诉我们一个状态的价值如何与其后续状态的价值相关联。

贝尔曼方程的本质是，当前状态s的总回报包含两部分，一部分是马上得到的奖励R(s)，第二部分是 $\gamma$ 乘以下一状态s'的最佳总回报
![](assets/Pasted%20image%2020230722101410.png)
例如从状态4开始，向左走：
$$
\begin{aligned}&Q(4,\leftarrow)\\&=0+(0.5)0+(0.5)^20+(0.5)^3100\\&=R(4)+(0.5)\begin{bmatrix}0+(0.5)0+(0.5)^2100\end{bmatrix}
\\&=R(4)+(0.5)\max_{a^{\prime}}Q(3,a^{\prime})\end{aligned}
$$

实际上，贝尔曼方程告诉我们，在当前状态下采取某个动作的价值与下一个状态的价值之间存在一个关系。通过不断迭代更新这个关系，最终将能够找到最佳策略，即在每个状态下都选择可以获得最大预期回报的动作。
## 随机马尔可夫决策

在随机马尔可夫过程中，执行动作a时，存在一定概率无法按照预期进行。以一个随机环境为例，我们向一个探测器发送左转指令，但由于未知的环境因素，左侧地面可能很滑，导致探测器滑向相反的方向，无法按照我们的指令执行。因此，我们获得的奖励也是随机的。

在随机强化学习问题中，目标的不是最大化回报，而是折扣奖励总和的平均值，即期望回报
$$\begin{aligned}\text{Expected Return}&=\text{Average}(R_1+\gamma R_2+\gamma^2R_3+\gamma^3R_4+\cdots)\\&=\text{E}[R_1+\gamma R_2+\gamma^2R_3+\gamma^3R_4+\cdots]\end{aligned}$$

随机马尔可夫决策中，贝尔曼方程改为：
![](assets/Pasted%20image%2020230722110235.png)
前面TensorFlow实例中，misstep_prob参数就表示失误概率。

# 连续状态空间应用

例如在自动控制汽车、飞机中，状态s都是连续状态空间，包含x,y,z轴坐标，及各轴方向上速度、角速度等，这都是连续状态空间

## 示例：登月器
登月器的任务是在适当的时候启动火力推进，将其安全降落到着陆台上。

**状态：**
![](assets/Pasted%20image%2020230722112131.png)
$x$ ：水平位置
$y$ ：垂直高度
$\dot{x}$ ：水平方向速度
$\dot{y}$ ：垂直方向速度
$\theta$ ：倾斜角，向左/右倾斜多少
$\dot{\theta}$ ：角速度
$l$ ：左脚落地
$r$ ：右脚落地

**动作：** 什么都不做，向下加速，向左加速，向右加速

**奖励函数：**
![](assets/Pasted%20image%2020230722113031.png)
到达着陆台：100-140
靠近/远离着陆台的额外奖励
坠毁：-100
软着陆：+100
腿着陆：+10
启动主推进器：-0.3
启动侧推进器：-0.03

**目标**是找到策略函数：
![](assets/Pasted%20image%2020230722113530.png)

## 学习状态值函数（强化学习中使用神经网络）


关键思想是我们要训练一个神经网络来计算或近似S,a的状态动作值函数Q,然后上我们洗择好的动作

学习算法的核心是我们将训练一个神经网络，该网络输入当前状态和当前动作并计算或近似的Q 。
我们将状态和动作放在一起作为输入，这个12个数字的列表，8个数字用于状态，然后是个数字是动作的单热编码。这是我们对神经网络的输入，称之为X。神经网络的工作就是输出Q。

因为我们稍后会使用神经网络训练算法，所以我还将Q作为训练神经网络逼近的目标值Y。强化学习与监督学习不同，但我们要做的不是输入状态并让它来输出动作。我们要做的是输入一个状态动作对，让它尝试输出Q,并在强化学习算法中使用神经网络。

![](assets/Pasted%20image%2020230722183834.png)
如果您可以在隐藏层和上层中选择适当的参数来训练神经网络，以便为您提供对Q的良好估计，那么每当您在月球着陆器处于某种状态s时，您就可以使用神经网络计算Q。对于所有四个动作，都可以计算Q，Q(s,nothing),Q(s,left),Q(s,main),Q(s,right)，最后，哪个具有最高值，就选择那个相应的动作。
例如，如果在这四个值中，Q(s,main)最大，那么将启动着陆器的主推进器。

所以问题就变成了，如何训练一个神经网络来输出Q?

方法是使用贝尔曼方程来创建包含大量示例×和y的训练集，然后我们使用监督学习，就像神经网络时所学的一样。使用监督学习，利用神经网络学习从x到y的映射，即从状态动作对到目标值Q的映射。

但是，如何获得具有x和y值的训练集，可以在其上训练神经网络？

这是贝尔曼方程：$Q(s,a)=R(s)+\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime})$ 。
![](assets/Pasted%20image%2020230722185112.png)我们将等式右边的值为神经网络的输出y，等式左边即为输入x。神经网络的工作是输入x，即输入状态动作对，并尝试准确预测右边的值。

在监督学习中，我们训练一个神经网络来学习一个函数f，它取决于参数W和B（神经网络各个层的参数），神经网络的最左端输入X，最右端放一些接近目标值y的东西。
![](assets/Pasted%20image%2020230722185443.png)
问题是，我们如何才能为新网络提供一个具有x和y的训练集来学习？

我们将使用着陆器，并尝试在其中随机执行不同的操作。通过在月球着陆器中尝试做不同的动作，我们将会得到很多例子，这些例子说明我们何时处于哪种状态并采取了哪些动作，可能是一个好的动作，也可能是一个坏的动作。然后，由于处于该状态，我们获得了一些奖励R(s)，接着，我们进入了某个新状态S'。$(s,a,R(s),s^{\prime})$ 我们称为元组。

![](assets/Pasted%20image%2020230722190456.png)

例如，也许有一次处于某个状态 $S^{(1)}$，采取了动作 $a^{(1)}$ ，得到了奖励 $R(S^{(1)})$ ，达到了个新状态 $S^{\prime(1)}$  ；
也许在不同的时间处于某个状态 $S^{(2)}$，采取了动作 $a^{(2)}$ ，得到了奖励 $R(S^{(2)})$ ，达到了个新状态 $S^{\prime(2)}$  ；以此类推 。
也许你已经这样做了10,000次甚至超过10,000次。这10,000个元组都是训练示例x，y。
![](assets/Pasted%20image%2020230722191729.png)
元组中的前两个将用于计算x，后两个将用于计算y。
例如，$x^1$ 就是 $x^{(1)}=(s^{(1)},a^{(1)})$ ；$y^1$ 将使用贝尔曼方程的右侧计算。
贝尔曼方程表示，当您输入 $s^{(1)},a^{(1)}$ 时，希望$Q(s^{(1)},a^{(1)})=R(s^{(1)})+\gamma\max_{a^{\prime}}Q(s^{\prime(1)},a^{\prime})$ 。$y^1$ 即等于 $R(s^{(1)})+\gamma\max_{a^{\prime}}Q(s^{\prime(1)},a^{\prime})$ 。注意，元组最后的两个元素提供了足够的信息来计算右侧，计算后这将是数字，比如12.5或17。 我们将数字保存为 $y^1$ 。这对 $x^1,y^1$ 成为了第一个训练示例。
![](assets/Pasted%20image%2020230722193800.png)
现在你可能想要知道 $Q(s^{\prime(1)},a^{\prime})$ 从哪里来，其实我们最初并不知道 $Q(s^{\prime(1)},a^{\prime})$ 是什么，但是当不知道Q函数是什么时，可以随机猜测一个初始值。刚开始这里的每一步Q都只是一些猜测，随着时间的推移，他们会变得更好，慢慢的逼近实际的Q函数值。

依此类推，直到你最终得到10,000个包含这些x,y对的训练示例。稍后，我们将采用这个训练集，其中x是具有12个特征的输入，而y只是数字。 我们将使用均方误差损失来训练一个新网络，以尝试将y预测为输入x的函数。

以上描述的只是我们将使用的算法的一部分，接下来看看它们是如何组合成一个学习Q函数的完整算法的。

首先，我们将采用我们的神经网络并随机初始化神经网络的所有参数。最初我们不知道Q函数是什么，我们只是完全随机的赋值。我们假设这个神经网络是我们对Q函数的初始随机猜测。这有点像你在训川练线性回归时随机初始化所有参数，然后使用梯度下降来改进参数。现在随机初始化没关系。重要的是算法是否可以慢慢改进参数以获得更好的估计。
![](assets/Pasted%20image%2020230722203836.png)
接下来，我们将重复执行以下操作；我们将对着陆器中采取动作，然后得到众多元组 $(s,a,R(s),s^{\prime})$ ， 我们要做的是存储这些元组的10,000个最新示例。当你运行这个算法时，你会看到着陆器有很多步骤，可能有几十万个步骤。为了确保我们最终不会使用过多的计算机内存，通常的做法是只记住我们在 MDP中看到的10,000个最近的此类元组。这种仅存储最近示例的技术在强化学习算法中称为回放缓冲区。
![](assets/Pasted%20image%2020230722203905.png)
日前，我们只是让月球着陆器随机飞行，有时会坠毁，有时不会坠毁，并根据学习算法经验获取这些元组。

有时我们会训练神经网络，为了训练神经网络，下面是我们要做的。

我们将查看我们保存的这10,000个最近的元组，并创建一个包含10,000个示例的训练集。
训练集需要很多对x和y。对于我们的训练示例，x将是s来自元组的 $(s,a)$ 这一部分。我们希望神经网络尝试预测的目标值是$\mathbf{y}=R(s)+\:\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime})$ 。我们如何获得Q的这个值？最初是我们随机初始化的这个神经网络，是一个猜测值。
![](assets/Pasted%20image%2020230722204559.png)
创建这10,000个训练示例后，我们将拥有训练示例 $x^1$ 、$y^1$ 到 $x^{10,000}$ 、$y^{10,000}$ 。我们将训练一个神经网络，我将把新的神经网络称为Qnew,这样 $Q_{new}(s,a)\approx y$ 。这正是训练那个神经网络 $f_{W,B}(x)\approx y$ 。现在，这个神经网络应该稍微更好地估计Q函数是什么，接着我们要做的是将Q设置为我们刚刚学习的新神经网络
![](assets/Pasted%20image%2020230722210052.png)
事实证明，如果您从真正随机猜测Q函数开始运行此算法，然后使用Bellman方 Set-new.程反复尝试改进Q函数的估计值。,9然后通过反复执行此操作，采取大量操作，训川练模型，这将改进您对Q函数的猜 Set-new. Jw,BX)XU测.,9对于您训川练的下一个模型，您现在对什么是Q函数有了更好的估计。那么你训练的下一个模型会更好当您更新Q等于Qnew时。然后下次训练 $Q(s^{\prime},a^{\prime})$ 时，将是一个更好的估计值。

当你在每次迭代中运行这个算法时，$Q(s^{\prime},a^{\prime})$ 有望成为Q函数的更好估计，这样当你运行算法足够长的时间时，这个估计值会越来越接近真实值，这样你就可以用它来挑选，希望是好的动作或 MDP。

这个算法称为DQN算法（Deep Q-Network），因为使用深度学习和神经网络训练模型来学习Q函数。
## 总2



问题关键是利用神经网络来计算或近似计算状态动作价值函数Q(s,a)

训练模型：输入为状态8个数字+动作4个量(0/1 one-hot编码)

![](file://C:/Users/Admin/Desktop/assets/f211ab8f8723467e9693b7e1be08e6a0.png?lastModify=1689933088)

对每个状态s，使用神经网络分别计算4个动作下的Q，选取Q值最大的动作

训练模型的关键问题是x,y样本值如何获取（x为s和a，y为Q，主要是y的获取）：把各种状态、各种动作都进行尝试。

![](file://C:/Users/Admin/Desktop/assets/97ade20186514ed0b7a26317ca12696e.png?lastModify=1689933088)

输入x是s加上a，共12个数，输出y是由R(s)和s'计算而来，是一个数字。损失函数采用均方误差损失，尝试预测y

**一开始，随机初始化神经网络，作为Q猜测（有点像在线性回归中，初始化参数，然后使用梯度下降来改进参数）**

**一开始，随机采取动作，存储最近的10000个不同s和a的元组信息 s,a,R(s),s'（回放缓冲区），y值计算中的Q最初是利用随机初始化的神经网络进行预测。然后利用这10000组样本数据训练神经网络模型（以x为输入，w b为参数的输出值去近似y），即新的Q预测，再利用新的Q去优化y值，再用于神经网络模型优化，从而不断提高对Q函数的猜测**

![](file://C:/Users/Admin/Desktop/assets/36cfb4a34f1a4d1c8f3c966027b8f7e0.png?lastModify=1689933088)

该算法称为DQN算法（Deep Q-Network DQ网络），使用深度学习训练Q函数

## 算法改进：改进的神经网络架构

改进前，神将网络只能输出一个值，要分别计算4次Q，Q(s,nothing),Q(s,left),Q(s,main),Q(s,right)各一次
![](assets/Pasted%20image%2020230722213200.png)

改进后，神经网络可以同时输出四个值，会更加有效，且更方便选择最大值，可直接用于贝尔曼方程计算
![](assets/Pasted%20image%2020230722213230.png)

## 算法改进：Epsilon Greedy 策略

ε-greedy策略是一种在强化学习中常用的策略，用于在探索（exploration）和利用（exploitation）之间进行权衡。该策略基于一个参数ε，用来决定代理在**选择动作时**是进行**随机探索**还是根据当前已学习的知识做出**最优动作**的利用。
![](assets/Pasted%20image%2020230722215349.png)

在强化学习中，ε-贪婪策略（方法2）是更常用的方法。在大多数情况下，我们选择使当前的Q(s, a)（神经网络训练出来的）最大化的动作a，即贪婪行动。只有在极少数情况下（偶尔）会随机选择动作，即探索行动。

方法1容易出现问题，因为由于随机初始化的原因，可能始终不会尝试一些动作，而这些动作可能会效果很好。为了解决这个问题，我们采用方法2，即使用ε-贪婪策略，对所有动作进行尝试。

当ε=0.05时，贪婪动作占95%，探索动作占5%。

通常，在学习刚开始时，我们将ε设置得较大，这样代理会频繁地选择随机动作，然后逐渐减小ε的值，因此随着时间的推移，代理不太可能随机采取行动，而更有可能根据Q函数来选择最优的动作。

在强化学习中，参数的选择非常关键，不合适的参数选择可能导致训练时间非常非常慢。
## 算法改进：小批量和软更新

小批量（mini-batches）可以提高算法运行速度，同样适用于监督学习，例如训练神经网络，或训练线性回归或逻辑回归模型。
软更新（soft updates）可以使强化学习更好的收敛
### 小批量

小批量是指在训练模型时，将训练数据划分为多个较小的批次进行模型参数的更新。相比于使用完整的训练集进行参数更新，小批量更新可以提高计算效率，并且有助于减少参数更新的方差。

当样本数量非常大时，例如有一亿个样本，如果梯度下降的每一步都需用全部样本计算平均值，那么算法会运行的很慢。使用小批量是每次梯度下降时，只关注一个batch的数据。使用小批量梯度下降的思想是不用再每次迭代中使用1亿个示例，而是选择一个较小的数，如1000，这将花费较少的时间。
![](assets/Pasted%20image%2020230722221956.png)

小批量梯度下降很容易在迭代时向着错误的方向前进，不可靠且有点嘈杂。但会趋向于全局最小值，每次迭代的计算成本要低得多，因此小批量梯度下降可以提高算法运行速度。
![](assets/Pasted%20image%2020230722222116.png)

在强化学习中同样可以应用mini-batch，如虽然缓冲存储区中有10000个样本元组，但每次训练神经网络模型时可以只选取1000个（一个batch）来进行训练
![](assets/Pasted%20image%2020230722223649.png)


### 软更新

软更新是指在深度强化学习的策略网络和目标网络之间进行参数更新时，采用一种平滑的更新方式。

在软更新中，每次更新时只更新一小部分的策略网络参数，而不是完全替换目标网络的参数。这样可以缓解训练过程中目标网络参数的剧烈变化，使得网络更新更加稳定，更好的收敛。

软更新通常使用一个超参数τ来控制更新的幅度，即通过对新参数和旧参数进行加权平均来更新目标网络的参数。
![](assets/Pasted%20image%2020230722223002.png)

# 强化学习发展现状

- 强化学习在模拟状态实现要比真实状态实现容易很多
- 应用要比监督学习和无监督学习少很多
- 前景很大，是机器学习的支柱之一

![](assets/Pasted%20image%2020230722223249.png)