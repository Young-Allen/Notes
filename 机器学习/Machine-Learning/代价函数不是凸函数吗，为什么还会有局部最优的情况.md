在深度学习中，我们通常使用梯度下降等基于一阶导数的优化算法来最小化代价函数，以更新神经网络中的权重。对于凸函数，只有一个全局最优解，并且不存在局部极小值点。但是，对于非凸函数，可能存在多个局部极小值点，而这些局部极小值点往往是梯度下降算法容易陷入的陷阱。

在深度学习中，神经网络的代价函数通常是非凸函数，因为神经网络的权重空间非常庞大复杂，并且代价函数通常包含多个权重变量，这使得代价函数的形状非常复杂。

因此，尽管神经网络的代价函数被设计为凸函数，但在实践中很难找到一个具有凸性质的代价函数，特别是当网络非常复杂时。这也是为什么梯度下降等优化算法可能会收敛到局部最小值而不是全局最小值的原因。

此外，即使代价函数是凸函数，也有可能出现梯度消失或梯度爆炸等问题，导致优化算法无法到达全局最优点。因此，在实践中，我们需要使用其他方法来避免陷入局部最小值，如加正则化、使用更好的初始化策略和网络结构、使用自适应学习率等。