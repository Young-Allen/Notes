在梯度下降中，初始化 w 和 b 的值通常是随机选取的。因此，虽然理论上有可能初始值恰好为极大值点，但这种情况并不常见。

对于大多数实际问题，参数空间通常是非常大的，并且很少有“平坦”的函数表面。这意味着，在参数空间内随机初始化 w 和 b 的值时，几乎肯定不会落在极大值点上。

此外，即使初始化值恰好为极大值点，梯度下降仍然可以找到全局最优解，只需选择合适的学习率和迭代次数即可。但是，如果初始值过于接近于局部最优解，则可能需要更多的时间和资源才能达到全局最优解。

因此，在实际应用中，通常使用随机初始化的方式来初始化 w 和 b 的值，并在多次运行中选择最优结果。此外，还可以使用一些常见的方法来初始化权重和偏置，如 Xavier 初始化、He 初始化等，这些方法可以更好地适应不同的网络结构和激活函数，从而提高训练的效率和性能。