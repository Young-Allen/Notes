# 分类决策树

![](assets/Pasted%20image%2020230715193726.png)

1. 在每个节点上如何选择特征进行拆分？
- 选择要拆分的特征以尝试最大化纯度
![](assets/Pasted%20image%2020230715170822.png)
2. 何时停止分支？
![](assets/Pasted%20image%2020230715171547.png)
- 分出类别
- 达到最大深度
- 当纯度低于阈值
- 节点的示例数量低于阈值

# 纯度

熵（entropy）的定义，它是衡量一组数据不纯度（混乱程度）的指标。
熵越小，纯度越高

![](assets/Pasted%20image%2020230715172451.png)

熵函数
![](assets/Pasted%20image%2020230715172601.png)
在决策树构建的过程中，我们希望通过选择特征使得熵最小化，即寻找能够最大程度减少数据集的纯度。通过计算每个特征的熵，并对其进行比较，我们可以选择最佳的划分特征来构建决策树的节点。

# 选择拆分信息增益
在决策树中，熵的减少称为信息增益（information gain）
信息增益是决策树算法中用于选择最佳划分特征的指标之一。它表示通过使用某个特征对数据集进行划分所能获得的整体熵减少量。

在计算信息增益时，首先需要计算划分前的数据集的熵（也称为初始熵）。然后，对于每个候选划分特征，并计算每个划分子集的熵。最后，通过计算划分前的熵与所有划分子集熵的加权平均来计算信息增益。


![](assets/Pasted%20image%2020230715183454.png)
其中，初始熵是对整个数据集进行计算得到的熵，子集熵是对每个划分子集进行计算得到的熵，子集样本数是每个划分子集中的样本数量，总样本数是整个数据集的样本总数。

未分支前有5个猫，5个狗，因此初始熵熵 $H(0.5)=1$ 。
分支后加权平均的熵为上图红色方框，未分支前的熵减去分支后加权平均的熵即为信息增益。

具体计算公式如下：

信息增益 = 初始熵 - ∑(子集熵 * 子集样本数 / 总样本数)
![](assets/Pasted%20image%2020230715191714.png)

信息增益表示通过使用某个特征划分数据集，分支后，**整体熵相对于初始熵的减少量**。我们希望选择具有**最大**信息增益的特征作为划分依据，因为它能够最大程度地**减少数据集的不确定性（混乱程度）**，提供更多有用的信息。

==总结：==熵代表混乱程度，熵越高，混乱程度越高；信息增益越大，熵减少的越大，说明从高熵减低到低熵，进而说明从高混乱程度到低混乱程度的变化。

# 决策树步骤
![](assets/Pasted%20image%2020230715193336.png)
从根节点开始，将所有的示例放置在根节点上。 
计算所有可能特征的信息增益，并选择具有最高信息增益的特征。 根据所选特征划分数据集，并创建决策树的左右分支。 
不断重复划分过程，直到满足停止准则为止： 
- 当一个节点是100%属于某一类别时 
- 当划分一个节点会导致树超过最大深度时 
- 额外划分带来的信息增益低于阈值时。

# 如何处理连续值特征

在决策树中处理连续性数值的特征通常有两种方法：二元切分和多元切分。

1. 二元切分（Binary Splitting）：通过选择一个阈值来将连续性数值特征进行二元切分。对于给定的连续性特征，可以选择一个合适的阈值将数据集划分为两个子集。例如，如果特征是年龄，可以选择一个年龄阈值，将数据分为小于等于阈值和大于阈值的两个子集。然后，根据这个划分继续构建决策树。
    
2. 多元切分（Multi-way Splitting）：与二元切分不同，多元切分将连续性数值特征划分为多个范围或区间。通过指定多个阈值或范围，将连续性特征划分为多个子集。例如，对于年龄特征，可以指定年龄范围(0-10岁, 11-20岁, 21-30岁, 等等)作为划分依据。然后，根据这个划分继续构建决策树。
    

无论是二元切分还是多元切分，决策树都会根据划分结果计算信息增益或其他评估指标，选择最佳的划分。

在示例中增加一列体重特征：
![](assets/Pasted%20image%2020230715195850.png)
对于体重，我们选取不同的阈值来分割，然后计算对应的信息增益
![](assets/Pasted%20image%2020230715200031.png)
当选择9时，信息增益最大，因此我们用是否大于9磅进行分支。


# 回归决策树
预测体重：
![](assets/Pasted%20image%2020230715201645.png)

![](assets/Pasted%20image%2020230715201734.png)

![](assets/Pasted%20image%2020230715201805.png)
方差 方差减小最大

# 有放回抽样样本

共十个样本，五个猫 五个狗，有放回的抽取10次，组成一个新的训练集。
![](assets/Pasted%20image%2020230715212515.png)


# 装袋决策树

有一个大小为m的训练集。

b等于1到B，我们将训练集放入袋子中，利用有放回抽样选出B次大小为m的新的训练集，我们在这些新的训练集中训练一个个决策树。
![](assets/Pasted%20image%2020230715222743.png)
选出一个新的训练集，训练一个决策树；选出另外一个新的训练集，在训练一个决策树，就这样选出B个训练集，训练出B个决策树。
B则代表决策树的数量， 已经构建了一个包含B棵不同树的集合，然后让这些树对预测的结果进行投票。

当B远大于100时，算法的性能并不会有显著的增加，只会降低计算速度。

这种从袋子中选出新的训练集创建的决策树称为装袋决策时，所以B代表的是bag。

弊端是，这种抽取训练集的方法，会使许多树在根节点或者根节点附近的某些节点使用相同的特征进行分支。因此对算法进行改进，尝试随机化每个节点的特征选择，使数之间变得更加不同这就是随机森林。
# 随机森林 

在每个节点上，在选择用于划分的特征时，如果有n个可用特征，则从中随机选择一个大小为k（k < n）的特征子集，并只允许算法从该特征子集中进行选择，从中选择出具有最高信息增益的特征进行分支。
当n很大时，K值的典型选择是它的平方根 ${\mathrm{k=\sqrt{n}}}$ 。

因此，这意味着训练集的任何微小变化都不太可能对整个随机森林算法的整体输出产生巨大影响。

# XGBoost
依然使用有放回的抽样方式，创建一个大小为m的新训练集。但不是以相等的（1/m）概率从所有示例中抽取，而是更有可能抽取那些先前训练树分类错误的示例。

**优势：**
助推树的开源实现 
快速高效的实现 
可选择良好的默认拆分标准和何时停止拆分的标准 
内置正则化，防止过拟合 
机器学习竞赛（如Kaggle竞赛）中极具竞争力的算法
# 决策树VS神经网络
![](assets/Pasted%20image%2020230716093526.png)


1. 数据集的特征：决策树在处理具有离散特征和类别特征的数据集时表现较好，而神经网络在处理具有连续特征和大量样本的数据集时通常更有效。
    
2. 数据集的大小：如果数据集非常大，神经网络通常能够通过深层次的学习来发现复杂的模式和关联，因此神经网络可能更适合用于大规模数据集。
    
3. 解释性要求：决策树提供了较好的可解释性，可以根据节点分裂规则和特征重要性进行解释。而神经网络通常被认为是“黑盒”模型，其内部权重和参数难以解释。
    
4. 预测性能需求：神经网络在某些高度复杂的问题上可能具有更强的预测性能，尤其是当数据集具有深层次的非线性关系时。集成树模型如随机森林和梯度提升树则在处理噪声和异常值方面表现较好。
    
5. 训练效率：决策树通常更容易训练和调优，而神经网络需要更多的计算资源和时间来训练和优化。
    
6. 预测速度：决策树是一种基于规则的模型，可以快速进行预测。相比之下，神经网络需要通过多层次的计算才能进行预测，因此在实时性要求较高的场景中可能不太适用。
    

最终的选择应该根据具体问题的需求和数据集特征来决定。在某些情况下，使用决策树和集成树（如随机森林、梯度提升树）能够提供较好的性能和解释性；而在其他情况下，神经网络可能会更适合。还可以尝试将两种算法结合起来，如使用神经网络进行特征提取，然后使用决策树进行分类或回归。